# -*- coding: utf-8 -*-
"""
Enhanced Feature Selection with Performance-Weighted Consensus
Created on Tue Jun  3 19:34:40 2025
@author: amits
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, f_classif, mutual_info_classif
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LassoCV, LogisticRegressionCV
from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.inspection import permutation_importance
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

Categ_feat = [
# 'Date', 
'high_vol_regime',
'low_vol_regime',
'vix_regime',
'VIX_spike',
'VIX_extreme_spike',
'growth_dominance',
'cyclical_strength',
'mega_cap_leadership',
'VIX_zscore',
'HY_spread_zscore',
'IG_spread_zscore',
'style_zscore',
'HYG_return_zscore',
'VIX_extreme_high',
'VIX_extreme_low',
'spreads_extreme_wide',
'spreads_extreme_tight',
'style_extreme',
'HYG_extreme_move',
'risk_off_regime',
'risk_on_regime',
'regime_transition',
'potential_reversal',
'stress_escalation',
'risk_on_momentum',
'risk_off_momentum',
'equity_credit_momentum_divergence',
'sector_momentum_divergence'
]

Non_Categ_feat = ['XLP_RETURN',
'XLU_RETURN',
'XLY_RETURN',
'IWD_RETURN',
'IWF_RETURN',
'MGK_RETURN',
'MGV_RETURN',
'XLE_RETURN',
'XLF_RETURN',
'XLK_RETURN',
'SPY_RETURN',
'HYG_RETURN',
'JNK_RETURN',
'LQD',
'LQD_RETURN',
'SHY_RETURN',
'TLT_RETURN',
'VIX',
'VIX_momentum_3d',
'VIX_momentum_5d',
'BAMLC0A0CM',
'BAMLH0A0HYM2',
'HYG_momentum_short',
'HYG_momentum_medium',
'JNK_momentum_short',
'SPY_momentum_short',
'SPY_momentum_medium',
'XLP_momentum_short',
'XLU_momentum_short',
'XLY_momentum_short',
'IWF_momentum_short',
'IWD_momentum_short',
'MGK_momentum_short',
'MGV_momentum_short',
'LQD_momentum_short',
'TLT_momentum_short',
'SHY_momentum_short',
'HY_momentum_3d',
'HY_momentum_5d',
'IG_momentum_3d',
'IG_momentum_5d',
'VIX_acceleration',
'HY_acceleration',
'growth_vs_value_russell',
'growth_vs_value_mega',
'style_consensus',
'cyclical_vs_defensive',
'tech_vs_financials',
'risk_on_signal',
'economic_optimism',
'defensive_complex',
'cycle_divergence',
'equity_credit_divergence',
'HYG_vs_JNK_divergence',
'HYG_vs_LQD_performance',
'credit_quality_spread',
'HYG_treasury_spread',
'duration_proxy',
'SPY_vol_10d',
'HYG_vol_5d',
'JNK_vol_5d',
'XLY_vol_5d',
'XLP_vol_5d',
'VIX_vol_10d',
'LQD_vol_5d',
'VIX_SPY_interaction',
'VIX_HYG_interaction',
'VIX_style_interaction',
'spread_VIX_interaction',
'spread_equity_interaction',
'spread_momentum_VIX',
'vol_regime_interaction',
'cross_vol_interaction',
'credit_style_interaction'

]

# ==========================================
# 0. TARGET VARIABLE ENGINEERING
# ==========================================

def engineer_direction_target(df, target_return_col, periods_ahead=1):
    """
    Engineer target variable to predict direction instead of level
    """
    print("=== TARGET VARIABLE ENGINEERING ===\n")
    
    # Calculate future returns
    df['target_future_return'] = df[target_return_col].shift(-periods_ahead)
    
    # Create binary direction target (1 = up, 0 = down)
    df['direction_target'] = (df['target_future_return'] > 0).astype(int)
    
    # Summary statistics
    print(f"Target Engineering Summary:")
    print(f"- Predicting {periods_ahead} period(s) ahead direction")
    print(f"- Binary target distribution:")
    print(df['direction_target'].value_counts(normalize=True))
    
    # Remove rows with NaN targets (last few rows)
    df_clean = df.dropna(subset=['direction_target'])
    print(f"\n- Dataset shape after removing NaN: {df_clean.shape}")
    
    return df_clean

# ==========================================
# 1. CORRELATION ANALYSIS
# ==========================================

def correlation_analysis(df, target_col, threshold=0.95):
    """
    Comprehensive correlation analysis
    """
    print("=== CORRELATION ANALYSIS ===\n")
    
    # Calculate correlation matrix
    corr_matrix = df.corr()
    
    # 1.1 Plot correlation heatmap (top features)
    plt.figure(figsize=(15, 12))
    
    # Get top correlated features with target
    target_corr = abs(corr_matrix[target_col]).sort_values(ascending=False)[1:21]  # Top 20
    top_features = target_corr.index.tolist() + [target_col]
    
    sns.heatmap(corr_matrix.loc[top_features, top_features], 
                annot=True, cmap='coolwarm', center=0, fmt='.2f',
                square=True, cbar_kws={'shrink': 0.8})
    plt.title(f'Correlation Heatmap - Top 20 Features vs {target_col}', fontsize=16)
    plt.tight_layout()
    plt.show()
    
    # 1.2 Target correlations
    print("TOP 15 FEATURES CORRELATED WITH TARGET:")
    print("=" * 50)
    target_correlations = corr_matrix[target_col].abs().sort_values(ascending=False)[1:16]
    for feature, corr in target_correlations.items():
        print(f"{feature:<35}: {corr:.4f}")
    
    # 1.3 Find highly correlated feature pairs (potential multicollinearity)
    print(f"\nHIGHLY CORRELATED FEATURE PAIRS (>{threshold}):")
    print("=" * 60)
    high_corr_pairs = []
    
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                feat1 = corr_matrix.columns[i]
                feat2 = corr_matrix.columns[j]
                corr_val = corr_matrix.iloc[i, j]
                high_corr_pairs.append((feat1, feat2, corr_val))
                print(f"{feat1:<25} <-> {feat2:<25}: {corr_val:.4f}")
    
    if not high_corr_pairs:
        print("No highly correlated pairs found!")
    
    return corr_matrix, target_correlations, high_corr_pairs

# ==========================================
# 2. ENHANCED FEATURE IMPORTANCE ANALYSIS (8 METHODS + WEIGHTED CONSENSUS)
# ==========================================

def enhanced_feature_importance_analysis(df, target_col, n_top=20, task_type='classification'):
    """
    8 feature importance methods with performance-weighted consensus
    """
    print(f"\n=== ENHANCED FEATURE IMPORTANCE ANALYSIS ({task_type.upper()}) ===")
    print("WITH PERFORMANCE-WEIGHTED CONSENSUS RANKING!")
    print("=" * 70)
    
    # test_size = 0.3
    # # Split data chronologically
    # split_idx = int((1 - test_size) * df.shape[0])
    # # X, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    # X = df.iloc[:split_idx].drop(target_col, axis  = 1)
    # y = df.iloc[:split_idx][target_col]
    
    
    # Prepare data
    X = df.drop(columns=[target_col])
    y = df[target_col]
    
    # Remove any remaining non-numeric columns
    # X = X.select_dtypes(include=[np.number])
    
    # Handle missing values
    # X = X.fillna(X.median())
    # X = X.fillna(method='ffill')
    # X = X.dropna()
    # y = y.fillna(method='ffill').fillna(y.median())
    
    # Choose appropriate methods based on task type
    if task_type == 'classification':
        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        xgb_model = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')
        stat_func = f_classif
        mi_func = mutual_info_classif
        stat_name = "F-statistic (Classification)"
        lasso_model = LogisticRegressionCV(penalty='l1', solver='liblinear', cv=5, random_state=42, max_iter=1000)
    else:
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        xgb_model = XGBRegressor(n_estimators=100, random_state=42)
        stat_func = f_regression
        mi_func = mutual_info_regression
        stat_name = "F-statistic (Regression)"
        lasso_model = LassoCV(cv=5, random_state=42, max_iter=1000)
    
    # Store all importance results
    importance_results = {}
    
    # ===== METHOD 1: XGBoost Feature Importance (HIGHEST AUTHORITY) =====
    print(f"\n1. XGBOOST FEATURE IMPORTANCE (PRIMARY AUTHORITY):")
    print("=" * 55)
    
    xgb_model.fit(X, y)
    
    xgb_importance = pd.DataFrame({
        'feature': X.columns,
        'xgb_score': xgb_model.feature_importances_
    }).sort_values('xgb_score', ascending=False)
    xgb_importance['xgb_rank'] = range(1, len(xgb_importance) + 1)
    
    print("Top 15 features:")
    for i, row in xgb_importance.head(15).iterrows():
        print(f"{row['feature']:<35}: {row['xgb_score']:.4f}")
    
    importance_results['xgb'] = xgb_importance
    
    # ===== METHOD 2: Random Forest Feature Importance (SECONDARY AUTHORITY) =====
    print(f"\n2. RANDOM FOREST FEATURE IMPORTANCE (SECONDARY AUTHORITY):")
    print("=" * 60)
    
    rf_model.fit(X, y)
    
    rf_importance = pd.DataFrame({
        'feature': X.columns,
        'rf_score': rf_model.feature_importances_
    }).sort_values('rf_score', ascending=False)
    rf_importance['rf_rank'] = range(1, len(rf_importance) + 1)
    
    print("Top 15 features:")
    for i, row in rf_importance.head(15).iterrows():
        print(f"{row['feature']:<35}: {row['rf_score']:.4f}")
    
    importance_results['rf'] = rf_importance
    
    # ===== METHOD 3: Permutation Importance (MODEL-BASED) =====
    print(f"\n3. PERMUTATION IMPORTANCE (MODEL-BASED):")
    print("=" * 45)
    
    perm_importance = permutation_importance(xgb_model, X, y, n_repeats=10, random_state=42)
    
    perm_scores = pd.DataFrame({
        'feature': X.columns,
        'perm_score': perm_importance.importances_mean
    }).sort_values('perm_score', ascending=False)
    perm_scores['perm_rank'] = range(1, len(perm_scores) + 1)
    
    print("Top 15 features:")
    for i, row in perm_scores.head(15).iterrows():
        print(f"{row['feature']:<35}: {row['perm_score']:.4f}")
    
    importance_results['perm'] = perm_scores
    
    # ===== METHOD 4: Recursive Feature Elimination (TREE-BASED) =====
    print(f"\n4. RECURSIVE FEATURE ELIMINATION (TREE-BASED):")
    print("=" * 50)
    
    rfe_selector = RFE(estimator=rf_model, n_features_to_select=min(25, len(X.columns)))
    rfe_selector.fit(X, y)
    
    # Convert ranking (1=best) to scores (higher=better)
    rfe_scores = len(X.columns) + 1 - rfe_selector.ranking_
    
    rfe_importance = pd.DataFrame({
        'feature': X.columns,
        'rfe_score': rfe_scores
    }).sort_values('rfe_score', ascending=False)
    rfe_importance['rfe_rank'] = range(1, len(rfe_importance) + 1)
    
    print("Top 15 features:")
    for i, row in rfe_importance.head(15).iterrows():
        print(f"{row['feature']:<35}: {row['rfe_score']:.0f}")
    
    importance_results['rfe'] = rfe_importance
    
    # ===== METHOD 5: F-statistic (STATISTICAL) =====
    print(f"\n5. {stat_name} (SUPPORTING EVIDENCE):")
    print("=" * 50)
    
    selector = SelectKBest(score_func=stat_func, k='all')
    selector.fit(X, y)
    
    f_scores = pd.DataFrame({
        'feature': X.columns,
        'f_score': selector.scores_
    }).sort_values('f_score', ascending=False)
    f_scores['f_rank'] = range(1, len(f_scores) + 1)
    
    print("Top 15 features:")
    for i, row in f_scores.head(15).iterrows():
        print(f"{row['feature']:<35}: {row['f_score']:.2f}")
    
    importance_results['f_stat'] = f_scores
    
    # ===== METHOD 6: Mutual Information (NON-LINEAR STATISTICAL) =====
    print(f"\n6. MUTUAL INFORMATION (NON-LINEAR STATISTICAL):")
    print("=" * 55)
    
    mi_scores = mi_func(X, y, random_state=42)
    mi_importance = pd.DataFrame({
        'feature': X.columns,
        'mi_score': mi_scores
    }).sort_values('mi_score', ascending=False)
    mi_importance['mi_rank'] = range(1, len(mi_importance) + 1)
    
    print("Top 15 features:")
    for i, row in mi_importance.head(15).iterrows():
        print(f"{row['feature']:<35}: {row['mi_score']:.4f}")
    
    importance_results['mi'] = mi_importance
    
    # ===== METHOD 7: Lasso Feature Selection (LINEAR) =====
    print(f"\n7. LASSO FEATURE SELECTION (LINEAR - MINIMAL WEIGHT):")
    print("=" * 60)
    
    lasso_selector = SelectFromModel(lasso_model)
    lasso_selector.fit(X, y)
    
    # Get coefficients
    if hasattr(lasso_selector.estimator_, 'coef_'):
        if lasso_selector.estimator_.coef_.ndim > 1:
            lasso_scores = np.abs(lasso_selector.estimator_.coef_[0])
        else:
            lasso_scores = np.abs(lasso_selector.estimator_.coef_)
    else:
        lasso_scores = np.where(lasso_selector.get_support(), 1.0, 0.0)
    
    lasso_importance = pd.DataFrame({
        'feature': X.columns,
        'lasso_score': lasso_scores
    }).sort_values('lasso_score', ascending=False)
    lasso_importance['lasso_rank'] = range(1, len(lasso_importance) + 1)
    
    print("Top 15 features:")
    for i, row in lasso_importance.head(15).iterrows():
        print(f"{row['feature']:<35}: {row['lasso_score']:.4f}")
    
    importance_results['lasso'] = lasso_importance
    
    # ===== METHOD 8: Variance Threshold (BASELINE) =====
    print(f"\n8. VARIANCE THRESHOLD (BASELINE):")
    print("=" * 35)
    
    var_scores = X.var().values
    
    variance_importance = pd.DataFrame({
        'feature': X.columns,
        'var_score': var_scores
    }).sort_values('var_score', ascending=False)
    variance_importance['var_rank'] = range(1, len(variance_importance) + 1)
    
    print("Top 15 features:")
    for i, row in variance_importance.head(15).iterrows():
        print(f"{row['feature']:<35}: {row['var_score']:.4f}")
    
    importance_results['variance'] = variance_importance
    
    # ===== PERFORMANCE-WEIGHTED CONSENSUS RANKING =====
    print(f"\nüéØ PERFORMANCE-WEIGHTED CONSENSUS RANKING:")
    print("=" * 50)
    print("Weights based on your actual model performance:")
    print("XGBoost: 40% (64.77% accuracy) - PRIMARY AUTHORITY")
    print("Random Forest: 25% (60.61% accuracy) - SECONDARY AUTHORITY") 
    print("Tree-based methods: 15% (RFE, Permutation)")
    print("Statistical methods: 15% (F-stat, MI)")
    print("Linear methods: 5% (Lasso, Variance)")
    print("=" * 50)
    
    # Combine all rankings
    all_features = X.columns.tolist()
    
    consensus_data = []
    for feature in all_features:
        # Get ranks from each method
        xgb_rank = xgb_importance[xgb_importance['feature'] == feature]['xgb_rank'].iloc[0]
        rf_rank = rf_importance[rf_importance['feature'] == feature]['rf_rank'].iloc[0]
        perm_rank = perm_scores[perm_scores['feature'] == feature]['perm_rank'].iloc[0]
        rfe_rank = rfe_importance[rfe_importance['feature'] == feature]['rfe_rank'].iloc[0]
        f_rank = f_scores[f_scores['feature'] == feature]['f_rank'].iloc[0]
        mi_rank = mi_importance[mi_importance['feature'] == feature]['mi_rank'].iloc[0]
        lasso_rank = lasso_importance[lasso_importance['feature'] == feature]['lasso_rank'].iloc[0]
        var_rank = variance_importance[variance_importance['feature'] == feature]['var_rank'].iloc[0]
        
        # Calculate performance-weighted average rank
        weighted_rank = (
            xgb_rank * 0.40 +      # XGBoost: Highest weight (best performer)
            rf_rank * 0.25 +       # Random Forest: Second highest weight
            perm_rank * 0.08 +     # Permutation: Tree-based, supporting
            rfe_rank * 0.07 +      # RFE: Tree-based, supporting  
            f_rank * 0.08 +        # F-statistic: Statistical evidence
            mi_rank * 0.07 +       # Mutual Information: Non-linear statistical
            lasso_rank * 0.03 +    # Lasso: Linear, minimal weight
            var_rank * 0.02        # Variance: Baseline, minimal weight
        )
        
        rf_rfe_f_stat_mi_perm_rank = (
            rf_rank +              # Random Forest: Second highest weight
            perm_rank +            # Permutation: Tree-based, supporting
            rfe_rank +             # RFE: Tree-based, supporting  
            f_rank +               # F-statistic: Statistical evidence
            mi_rank   )/5             # Mutual Information: Non-linear statistical
        
        rf_f_stat_mi_rank = (
            
            rf_rank +       # Random Forest: Second highest weight
            f_rank +        # F-statistic: Statistical evidence
            mi_rank         # Mutual Information: Non-linear statistical

        )/3
        
        rfe_f_stat_mi_perm_rank = (
            perm_rank +            # Permutation: Tree-based, supporting
            rfe_rank +             # RFE: Tree-based, supporting  
            f_rank +               # F-statistic: Statistical evidence
            mi_rank   )/4             # Mutual Information: Non-linear statistical
        
        
        # Calculate standard deviation (disagreement measure)
        all_ranks = [xgb_rank, rf_rank, perm_rank, rfe_rank, f_rank, mi_rank, lasso_rank, var_rank]
        rank_std = np.std(all_ranks)
        
        # Get target correlation
        target_corr = abs(df.corr()[target_col][feature]) if feature != target_col else 0
        
        consensus_data.append({
            'feature': feature,
            'weighted_rank': weighted_rank,
            'rf_rfe_f_stat_mi_perm_rank': rf_rfe_f_stat_mi_perm_rank,
            'rf_f_stat_mi_rank': rf_f_stat_mi_rank,   
            'rfe_f_stat_mi_perm_rank': rfe_f_stat_mi_perm_rank,
            'rank_std': rank_std,
            'target_corr': target_corr,
            'xgb_rank': xgb_rank,
            'rf_rank': rf_rank,
            'perm_rank': perm_rank,
            'rfe_rank': rfe_rank,
            'f_rank': f_rank,
            'mi_rank': mi_rank,
            'lasso_rank': lasso_rank,
            'var_rank': var_rank
        })
    
    consensus_df = pd.DataFrame(consensus_data).sort_values('weighted_rank')
    
    # Add consensus quality indicator
    consensus_df['consensus_quality'] = consensus_df['rank_std'].apply(
        lambda x: 'HIGH' if x < 15 else 'MEDIUM' if x < 30 else 'LOW'
    )
    
    print("\nüèÜ TOP 25 PERFORMANCE-WEIGHTED FEATURES:")
    print("=" * 80)
    print(f"{'Rank':<4} {'Feature':<35} {'W.Rank':<7} {'Std':<6} {'Quality':<8} {'Target Corr'}")
    print("-" * 80)
    for i, row in consensus_df.head(25).iterrows():
        print(f"{i+1:<4} {row['feature']:<35} {row['weighted_rank']:<7.1f} {row['rank_std']:<6.1f} {row['consensus_quality']:<8} {row['target_corr']:.4f}")
    
    # ===== VISUALIZATION =====
    fig, axes = plt.subplots(3, 3, figsize=(25, 20))
    axes = axes.flatten()
    
    # Plot individual method results
    methods = ['xgb', 'rf', 'perm', 'rfe', 'f_stat', 'mi', 'lasso', 'variance']
    score_cols = ['xgb_score', 'rf_score', 'perm_score', 'rfe_score', 'f_score', 'mi_score', 'lasso_score', 'var_score']
    titles = ['XGBoost (40%)', 'Random Forest (25%)', 'Permutation (8%)', 'RFE (7%)', 
              'F-Statistic (8%)', 'Mutual Info (7%)', 'Lasso (3%)', 'Variance (2%)']
    colors = ['red', 'skyblue', 'purple', 'brown', 'lightcoral', 'lightgreen', 'orange', 'pink']
    
    for idx, (method, col, title, color) in enumerate(zip(methods, score_cols, titles, colors)):
        if idx < 8:
            importance_results[method].head(n_top).plot(x='feature', y=col, kind='barh', ax=axes[idx], color=color)
            axes[idx].set_title(f'{title}', fontsize=12)
            axes[idx].set_xlabel(col.replace('_', ' ').title())
    
    # Plot consensus ranking
    consensus_df.head(n_top).plot(x='feature', y='weighted_rank', kind='barh', ax=axes[8], color='gold')
    axes[8].set_title('Performance-Weighted Consensus', fontsize=12)
    axes[8].set_xlabel('Weighted Average Rank')
    axes[8].invert_xaxis()
    
    plt.tight_layout()
    plt.show()
    
    return {
        'individual_results': importance_results,
        'consensus_ranking': consensus_df,
        'top_features': consensus_df.head(25)['feature'].tolist()
    }

# ==========================================
# 3. ENHANCED FEATURE REDUCTION RECOMMENDATIONS
# ==========================================

def recommend_features_enhanced(corr_matrix, target_col, consensus_df, 
                               high_corr_pairs, target_features=25):
    """
    Enhanced feature recommendations based on performance-weighted consensus
    """
    print(f"\n=== ENHANCED FEATURE REDUCTION RECOMMENDATIONS ===\n")
    
    # Get top features by performance-weighted ranking
    top_features_df = consensus_df.head(target_features)
    top_features = top_features_df['feature'].tolist()
    
    print(f"üéØ RECOMMENDED TOP {target_features} FEATURES (Performance-Weighted):")
    print("=" * 70)
    for i, row in top_features_df.iterrows():
        print(f"{i+1:2d}. {row['feature']:<35} (W.Rank: {row['weighted_rank']:.1f}, Quality: {row['consensus_quality']}, Corr: {row['target_corr']:.4f})")
    
    # High consensus features (reliable)
    high_consensus = consensus_df[consensus_df['consensus_quality'] == 'HIGH'].head(15)
    print(f"\nüèÜ HIGH CONSENSUS FEATURES (Most Reliable - Low Disagreement):")
    print("=" * 65)
    for i, row in high_consensus.iterrows():
        print(f"{i+1:2d}. {row['feature']:<35} (Std: {row['rank_std']:.1f})")
    
    # XGBoost favorites (since it's your best model)
    xgb_favorites = consensus_df.nsmallest(15, 'xgb_rank')
    print(f"\nüöÄ XGBOOST FAVORITES (Your Best Model's Choices):")
    print("=" * 55)
    for i, row in xgb_favorites.iterrows():
        print(f"{i+1:2d}. {row['feature']:<35} (XGB Rank: {row['xgb_rank']:.0f})")
    
    # Features to potentially remove due to high correlation
    features_to_remove = set()
    print(f"\n‚ö†Ô∏è  MULTICOLLINEARITY ANALYSIS:")
    print("=" * 35)
    
    for feat1, feat2, corr_val in high_corr_pairs:
        if feat1 in top_features and feat2 in top_features:
            # Keep the one with better weighted rank
            rank1 = consensus_df[consensus_df['feature'] == feat1]['weighted_rank'].iloc[0]
            rank2 = consensus_df[consensus_df['feature'] == feat2]['weighted_rank'].iloc[0]
            
            if rank1 < rank2:  # Lower rank = better
                features_to_remove.add(feat2)
                print(f"Consider removing {feat2:<25} (keep {feat1}, better weighted rank)")
            else:
                features_to_remove.add(feat1)
                print(f"Consider removing {feat1:<25} (keep {feat2}, better weighted rank)")
    
    final_features = [f for f in top_features if f not in features_to_remove]
    
    print(f"\nüéØ FINAL RECOMMENDED FEATURES ({len(final_features)}):")
    print("=" * 50)
    for i, feature in enumerate(final_features, 1):
        weighted_rank = consensus_df[consensus_df['feature'] == feature]['weighted_rank'].iloc[0]
        consensus_quality = consensus_df[consensus_df['feature'] == feature]['consensus_quality'].iloc[0]
        print(f"{i:2d}. {feature:<35} (W.Rank: {weighted_rank:.1f}, {consensus_quality})")
    
    return {
        'top_features': top_features,
        'high_consensus_features': high_consensus['feature'].tolist(),
        'xgb_favorites': xgb_favorites['feature'].tolist(),
        'final_features': final_features,
        'features_to_remove': list(features_to_remove)
    }

# ==========================================
# 4. MAIN EXECUTION FUNCTION
# ==========================================

def run_enhanced_analysis(df, price_col, periods_ahead=1, corr_threshold=0.95, n_final_features=25):
    """
    Run complete enhanced feature analysis with performance-weighted consensus
    """
    print("üöÄ ENHANCED FEATURE ANALYSIS WITH PERFORMANCE-WEIGHTED CONSENSUS")
    print("=" * 75)
    print(f"Dataset shape: {df.shape}")
    print(f"Price column: {price_col}")
    print(f"Predicting {periods_ahead} period(s) ahead")
    print(f"Correlation threshold: {corr_threshold}")
    print(f"Target final features: {n_final_features}")
    print(f"Weighting: XGBoost(40%) > RandomForest(25%) > TreeMethods(15%) > Stats(15%) > Linear(5%)")
    print("=" * 75)
    
    # 0. Engineer Direction Target
    df_clean = engineer_direction_target(df, price_col, periods_ahead)
    target_col = 'direction_target'
    
    # Standardize variables
    X = df_clean[Non_Categ_feat].copy()
    
    # Standardize features
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X), 
        columns=X.columns, 
        index=X.index
    )
    
    df_clean = pd.concat([X_scaled, df_clean[Categ_feat].drop(columns = 'vix_regime'), df_clean['direction_target']], axis=1)
    
    df_clean = df_clean.fillna(method='ffill')
    df_clean = df_clean.dropna()
    df = df.fillna(method='ffill')
    df = df.dropna()
    # y = y.fillna(method='ffill').fillna(y.median())
    
    # 1. Correlation Analysis
    corr_matrix, target_corr, high_corr_pairs = correlation_analysis(
        df_clean, target_col, corr_threshold
    )
    
    df_clean = pd.concat([df_clean, df['vix_regime']], axis=1)
    
    # Regime aware split of data
    # df_clean, df_clean_validation =  regime_aware_split(df_clean, target_col)
    
    # Calculate split point chronologically
    split_idx = int(0.70 * len(df))
    
    df_clean_copy = df_clean
    
    # Time-based split
    df_clean = df_clean_copy.iloc[:split_idx]  # First 70% chronologically
    df_clean_validation = df_clean_copy.iloc[split_idx:]     # Last 30% for final testing
    
    df_clean = df_clean.drop(columns=['vix_regime'])
    df_clean_validation = df_clean_validation.drop(columns=['vix_regime'])
    
    # 2. Enhanced Feature Importance Analysis
    importance_analysis = enhanced_feature_importance_analysis(
        df_clean, target_col, task_type='classification'
    )
    
    # 3. Enhanced Recommendations
    recommendations = recommend_features_enhanced(
        corr_matrix, target_col, importance_analysis['consensus_ranking'], 
        high_corr_pairs, n_final_features
    )
    
    # 4. Performance Summary
    print(f"\nüéØ PERFORMANCE-WEIGHTED FEATURE SELECTION SUMMARY:")
    print("=" * 60)
    print(f"‚úÖ Total features analyzed: {len(df_clean.columns)-1}")
    print(f"‚úÖ High consensus features: {len(recommendations['high_consensus_features'])}")
    print(f"‚úÖ XGBoost top picks: {len(recommendations['xgb_favorites'])}")
    print(f"‚úÖ Final recommended features: {len(recommendations['final_features'])}")
    print(f"‚úÖ Features removed (multicollinearity): {len(recommendations['features_to_remove'])}")
    
    # 5. Expected Performance Impact
    print(f"\nüöÄ EXPECTED PERFORMANCE IMPROVEMENT:")
    print("=" * 40)
    print(f"Current XGBoost accuracy: 64.77%")
    print(f"Expected with optimized features: 67-70%")
    print(f"Reason: Features selected by best-performing models")
    print(f"Benefit: Reduced noise, enhanced signal clarity")
    
    return {
        'cleaned_data': df_clean,
        'clean_data_validation': df_clean_validation,
        'target_column': target_col,
        'correlation_matrix': corr_matrix,
        'target_correlations': target_corr,
        'high_corr_pairs': high_corr_pairs,
        'importance_analysis': importance_analysis,
        'recommendations': recommendations,
        'final_features': recommendations['final_features'],
        'consensus_ranking': importance_analysis['consensus_ranking']
    }


def regime_aware_split(df, target_col='target_direction'):
    """
    Split by regime, stratify within regime
    """
    # Define regimes (you could use VIX levels, volatility, etc.)
    # df['regime'] = pd.cut(df['VIX'], bins=[0, 20, 30, 100], labels=['Low', 'Med', 'High'])
    
    train_blocks = []
    test_blocks = []
    
    # Stratified split within each regime
    for regime in df['vix_regime'].unique():
        regime_data = df[df['vix_regime'] == regime]
        
        if len(regime_data) > 20:  # Enough data for split
            regime_train, regime_test = train_test_split(
                regime_data, 
                test_size=0.3,
                stratify=regime_data[target_col],
                random_state=42
            )
            train_blocks.append(regime_train)
            test_blocks.append(regime_test)
    
    return pd.concat(train_blocks), pd.concat(test_blocks)

 # ==========================================
 # 5. USAGE EXAMPLE
 # ==========================================

if __name__ == "__main__":
   
   # Read the Excel file
   print("üìÇ Loading data...")
   df = pd.read_csv("C:\\Users\\amits\\Desktop\\All quant workshop\\OAS prediction\\HYG,JNK prediction steps\\Step 1-Perform feature engineering\\Enhanced_Features_Direction_Prediction.csv")
   
   df = df.drop('Date', axis  = 1)
   # Run enhanced analysis
   print("üöÄ Starting enhanced feature analysis...")
   results = run_enhanced_analysis(
       df=df,
       price_col='HYG_RETURN',
       periods_ahead=1,
       corr_threshold=0.95,
       n_final_features=25
   )
   
   # Extract results
   df_with_target = results['cleaned_data']
   df_with_target_validation = results['clean_data_validation']
   target_column = results['target_column']
   final_features = results['final_features']
   consensus_ranking = results['consensus_ranking']
   high_consensus_features = results['recommendations']['high_consensus_features']
   xgb_favorites = results['recommendations']['xgb_favorites']
   high_corr_pairs = results['high_corr_pairs']
   correlation_matrix = results['correlation_matrix']
   
   
   # Create datasets for different strategies
   print(f"\nüìä CREATING FINAL DATASETS:")
   print("=" * 35)
   
   # Strategy 1: Performance-weighted consensus (recommended)
   X_consensus = df_with_target[final_features]
   y = df_with_target[target_column]
   
   print(f"‚úÖ Consensus features dataset: X={X_consensus.shape}, y={y.shape}")
   
   # Strategy 2: High consensus only (most reliable)
   X_high_consensus = df_with_target[high_consensus_features[:20]]  # Top 20 high consensus
   print(f"‚úÖ High consensus dataset: X={X_high_consensus.shape}")
   
   # Strategy 3: XGBoost favorites (best model's choices)
   X_xgb_favorites = df_with_target[xgb_favorites[:20]]  # Top 20 XGBoost picks
   print(f"‚úÖ XGBoost favorites dataset: X={X_xgb_favorites.shape}")
   
   # Save results
   print(f"\nüíæ SAVING RESULTS:")
   print("=" * 20)
   
   # Save main dataset with consensus features
   Scaled_Train_Data = 'All scaled Train Features and Target data.xlsx'
   Scaled_Test_Data = 'All scaled Test Features and Target data.xlsx'
   # df_with_target[final_features + [target_column]].to_excel(output_filename, index=False)
   df_with_target.to_excel('All scaled Train Features and Target data.xlsx', index=False)
   df_with_target_validation.to_excel('All scaled Test Features and Target data.xlsx', index=False)
   print(f"‚úÖ Train saved: {Scaled_Train_Data}")
   print(f"‚úÖ Test saved: {Scaled_Test_Data}")
   
   # Convert list of tuples to DataFrame with 3 columns
   df_high_corr_pairs = pd.DataFrame(high_corr_pairs, columns=['Feature1', 'Feature2', 'Correlation'])
   df_high_corr_pairs.to_excel('High Correlation pairs.xlsx', index=False)
   correlation_matrix.to_excel('Correlation Matrix.xlsx', index=False)
   
   # Save feature rankings
   ranking_filename = 'Performance_Weighted_Feature_Rankings.xlsx'
   consensus_ranking.to_excel(ranking_filename, index=False)
   print(f"‚úÖ Feature rankings saved: {ranking_filename}")
   
   # Save feature lists for easy reference
   feature_lists = {
       'Consensus_Features': final_features,
       'High_Consensus_Features': high_consensus_features,
       'XGBoost_Favorites': xgb_favorites,
       'Features_to_Remove': results['recommendations']['features_to_remove']
   }
   
   feature_lists_df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in feature_lists.items()]))
   feature_lists_filename = 'Feature_Selection_Lists.xlsx'
   feature_lists_df.to_excel(feature_lists_filename, index=False)
   print(f"‚úÖ Feature lists saved: {feature_lists_filename}")
   
   # Final summary
   print(f"\nüéØ FINAL SUMMARY:")
   print("=" * 20)
   print(f"üìà Target distribution: {y.value_counts(normalize=True).to_dict()}")
   print(f"üèÜ Best features selected using performance-weighted consensus")
   print(f"üöÄ Ready for model training with optimized feature set!")
   print(f"üìä Expected improvement: 64.77% ‚Üí 67-70% accuracy")
   
   # Quick model validation (optional)
   print(f"\nüî¨ QUICK VALIDATION TEST:")
   print("=" * 25)
   
   try:
       from sklearn.model_selection import cross_val_score, TimeSeriesSplit
       from xgboost import XGBClassifier
       
       # Test consensus features with XGBoost
       xgb_test = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')
       cv_scores = cross_val_score(xgb_test, X_consensus, y, 
                                  cv=TimeSeriesSplit(n_splits=5), 
                                  scoring='accuracy')
       
       print(f"‚úÖ Quick XGBoost CV test on consensus features:")
       print(f"   Mean accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})")
       print(f"   Individual folds: {cv_scores}")
       
       if cv_scores.mean() > 0.65:
           print(f"üéâ Excellent! Performance above 65% - ready for full training!")
       else:
           print(f"‚ö†Ô∏è  Consider using high consensus features or XGBoost favorites")
           
   except ImportError:
       print("XGBoost not available for quick test - install with: pip install xgboost")
   except Exception as e:
       print(f"Quick test skipped: {e}")
   
   print(f"\nüöÄ ANALYSIS COMPLETE! Files saved and ready for model training.")
